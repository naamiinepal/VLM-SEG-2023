{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/manish/sedoc/biomedclip/lightning-hydra-starter\")\n",
    "\n",
    "from src.prompts.generator.mmq import MMQVQARAD\n",
    "\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\n",
    "    \"/home/manish/sedoc/biomedclip/lightning-hydra-starter/src/prompts/generator/mmq/data/images/tumor.png\",\n",
    "    cv2.IMREAD_GRAYSCALE,\n",
    ")\n",
    "\n",
    "if img is None:\n",
    "    raise FileNotFoundError(\"Image file does not exist\")\n",
    "\n",
    "que = \"What organ is shown in the image?\"\n",
    "\n",
    "vqa = MMQVQARAD()\n",
    "\n",
    "# image argument must be grayscale but can either be Tensor or numpy array of shape: (H, W)\n",
    "ans = vqa(img, que)\n",
    "print(ans)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.prompts.generator.mmq import MMQPathVQA\n",
    "\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\n",
    "    \"/home/manish/sedoc/biomedclip/lightning-hydra-starter/data/gland_segmentation_of_haematoxylin_and_eosin/train/images/train_1.bmp\",\n",
    ")\n",
    "vqa = MMQPathVQA()\n",
    "que = \"What is the color?\"\n",
    "\n",
    "# Iteraring through all the images in the folder\n",
    "for file in glob.glob(\n",
    "    \"/home/manish/sedoc/biomedclip/lightning-hydra-starter/data/gland_segmentation_of_haematoxylin_and_eosin/testB/images/*.bmp\"\n",
    "):\n",
    "    img = cv2.imread(file)\n",
    "    # image argument can be either Tensor: (3, H, W) or numpy array: (H, W, 3)\n",
    "    ans = vqa(img, que)\n",
    "    print(ans)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(\"Image file does not exist\")\n",
    "\n",
    "if img is None:\n",
    "    raise FileNotFoundError(\"Image file does not exist\")\n",
    "\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.0917e-02,  2.2895e-02, -2.7591e-02, -4.5788e-02, -1.1200e-01,\n",
      "        -1.1005e-01,  1.9753e-02,  1.6592e-01, -2.8005e-02,  3.3489e-03,\n",
      "        -2.6764e-02, -2.7254e-02,  1.2205e-01, -2.3496e-01,  1.2045e-01,\n",
      "         2.6418e-01,  1.0797e-01, -1.0283e-01, -1.7212e-01, -1.8607e-02,\n",
      "         2.0640e-02, -1.1911e-01,  1.2875e-01, -1.3516e-01, -1.3559e-01,\n",
      "         3.0904e-02,  1.0904e-01,  4.7330e-02,  6.1073e-03, -8.0883e-02,\n",
      "        -3.4374e-02, -6.3391e-02,  7.2105e-02, -9.4911e-02,  1.7334e-01,\n",
      "         1.3847e-01,  2.1367e-01, -1.1600e-01,  7.8969e-02,  8.7176e-02,\n",
      "         2.3097e-01, -9.2725e-02, -1.9136e-03,  2.1389e-02,  2.3492e-01,\n",
      "        -4.2502e-02,  6.2238e-02, -9.2287e-02, -2.5385e-02, -1.0932e-01,\n",
      "         4.0123e-02, -8.7505e-02, -1.5011e-04,  1.5180e-01, -7.8074e-02,\n",
      "        -4.3810e-03, -3.6527e-03, -1.7006e-01, -2.4982e-02,  1.0879e-01,\n",
      "         1.7464e-01,  7.0952e-02, -1.7283e-01, -7.1758e-02,  9.4950e-02,\n",
      "        -5.0678e-02,  1.8603e-02,  1.4882e-01, -1.8761e-01, -1.3630e-01,\n",
      "        -1.5491e-01,  4.4096e-02, -1.1330e-01,  7.3038e-02,  8.8313e-02,\n",
      "         4.6638e-02,  1.0033e-01,  1.8194e-01,  2.2995e-02, -5.0178e-02,\n",
      "        -1.7199e-01, -1.6352e-01, -1.4647e-01,  6.8797e-02,  7.9509e-02,\n",
      "         8.6527e-02, -8.8018e-02, -5.6259e-02, -3.2398e-02, -1.9864e-01,\n",
      "         2.6141e-01,  2.2713e-01,  9.3937e-02,  3.1889e-01,  1.5944e-01,\n",
      "        -9.8650e-03, -7.3246e-02, -3.3931e-01,  5.8499e-03, -6.3306e-03,\n",
      "         1.0321e-02,  2.9477e-02,  8.1986e-02,  9.1614e-02, -2.6659e-02,\n",
      "        -2.0271e-01,  4.5103e-02,  1.1968e-01, -3.5705e-02,  1.5545e-01,\n",
      "        -2.5499e-02, -1.3209e-01,  3.1122e-02,  1.2323e-02, -4.9712e-02,\n",
      "         1.7965e-01,  5.8407e-02,  1.0981e-01,  2.1154e-01, -1.3939e-01,\n",
      "         3.1171e-02,  5.9683e-02, -2.1035e-01,  2.6561e-02,  2.6974e-02,\n",
      "         5.6086e-02, -1.0880e-01, -2.7117e-02,  1.9110e-02, -3.0843e-02,\n",
      "         9.8795e-02,  1.4655e-01,  2.1088e-01,  3.9212e-03, -3.7420e-02,\n",
      "        -1.3778e-01,  1.0054e-01,  1.6733e-01, -2.7566e-01, -2.2648e-02,\n",
      "         1.9928e-01,  6.0452e-02, -6.0191e-02,  6.7123e-02, -6.2481e-02,\n",
      "        -4.9655e-02, -1.8919e-01, -3.3163e-02, -6.1311e-02,  1.0831e-01,\n",
      "         1.9236e-01, -2.2502e-02, -1.1320e-01, -5.3088e-02, -3.3264e-02,\n",
      "         5.3917e-02, -1.8456e-02,  8.8619e-02, -4.3869e-02, -1.7700e-01,\n",
      "         1.9088e-01,  4.9837e-02, -2.2244e-01, -1.9975e-02,  2.8275e-02,\n",
      "        -9.7457e-02,  1.3416e-01, -9.5547e-03, -1.1617e-01,  1.2546e-01,\n",
      "         1.5888e-01,  2.2285e-01,  9.4242e-02,  6.1241e-02,  6.0564e-02,\n",
      "         1.1835e-01, -1.0499e-01,  1.0098e-01,  1.5192e-01,  2.3779e-02,\n",
      "         9.9150e-02, -1.1358e-01,  4.0483e-03, -2.6200e-02,  4.0285e-02,\n",
      "        -1.1046e-01,  2.1565e-01, -2.5788e-01,  6.9867e-03, -1.7874e-01,\n",
      "         1.1460e-01, -1.2305e-03, -5.8406e-02,  1.1774e-01, -4.6630e-02,\n",
      "         2.7718e-02,  2.1804e-02, -7.2172e-02, -1.0961e-01, -7.6081e-02])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.seed()\n",
    "cosine_similarity_loss = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "x1 = torch.randn(200, 64)\n",
    "x2 = torch.randn(200, 64)\n",
    "print(cosine_similarity_loss(x1, x2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GrayScale Imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# Read raw image from online\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image = cv2.imread(\n",
    "    \"/home/manish/sedoc/biomedclip/lightning-hydra-starter/data/sdm_camus/2_chamber_end_diastole/images/testing/patient0001_2CH_ED_testing.png\",\n",
    "    cv2.IMREAD_COLOR,\n",
    ")\n",
    "print(image.shape)\n",
    "# Define transformations\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=256, height=256),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Normalize(),\n",
    "        A.ToGray(always_apply=True),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformed = transform(image=image)\n",
    "transformed_image = transformed[\"image\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Enterprise/manish/venvs/biomedclip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 256, 256]) torch.Size([16, 1, 256, 256])\n",
      "tensor(2.1165)\n"
     ]
    }
   ],
   "source": [
    "from monai.losses.dice import *  # NOQA\n",
    "import torch\n",
    "from monai.losses.dice import DiceCELoss\n",
    "B, C, H, W = 16, 4, 256, 256\n",
    "input = torch.rand(B, C, H, W)\n",
    "target = torch.randint(low=0, high=C - 1, size=(B,1, H, W)).long()\n",
    "self = DiceCELoss(reduction='mean',to_onehot_y=True)\n",
    "print(input.shape, target.shape )\n",
    "loss = self(input, target)\n",
    "print(loss)\n",
    "assert np.broadcast_shapes(loss.shape, input.shape) == input.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\n",
    "    \"/home/manish/masks.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "np.unique(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer(\n",
    "    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n",
    "    local_dir=\"biomed-clip-share\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from timm.models import VisionTransformer\n",
    "from open_clip.hf_model import HFTextEncoder, ClsPooler\n",
    "import torch\n",
    "\n",
    "\n",
    "def forward_vit(ViT, x, output_hidden_states: bool = True):\n",
    "    x = ViT.patch_embed(x)\n",
    "    x = ViT._pos_embed(x)\n",
    "    x = ViT.norm_pre(x)\n",
    "\n",
    "    hidden_states = [x]\n",
    "\n",
    "    for block in ViT.blocks:\n",
    "        x = block(x)\n",
    "        hidden_states.append(x)\n",
    "    x = ViT.norm(x)\n",
    "\n",
    "    if ViT.global_pool:\n",
    "        x = (\n",
    "            x[:, ViT.num_prefix_tokens :].mean(dim=1)\n",
    "            if ViT.global_pool == \"avg\"\n",
    "            else x[:, 0]\n",
    "        )\n",
    "    x = ViT.fc_norm(x)\n",
    "    x = ViT.head(x)\n",
    "\n",
    "    if output_hidden_states:\n",
    "        return x, hidden_states\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def forward_bert(bert: HFTextEncoder, x, output_hidden_states: bool = True):\n",
    "    attn_mask = (x != bert.config.pad_token_id).long()\n",
    "    out = bert.transformer(\n",
    "        input_ids=x, attention_mask=attn_mask, output_hidden_states=output_hidden_states\n",
    "    )\n",
    "    pooled_out = bert.pooler(out, attn_mask)\n",
    "    print(out.hidden_states[-1][:, 0])\n",
    "    print(pooled_out[0])\n",
    "    projected = bert.proj(pooled_out)\n",
    "\n",
    "    seq_len = out.last_hidden_state.shape[1]\n",
    "    tokens = (\n",
    "        out.last_hidden_state[\n",
    "            :, torch.arange(seq_len) != bert.pooler.cls_token_position, :\n",
    "        ]\n",
    "        if type(bert.pooler) == ClsPooler\n",
    "        else out.last_hidden_state\n",
    "    )\n",
    "\n",
    "    if bert.output_tokens:\n",
    "        return projected, tokens\n",
    "    return projected, out.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess_val = transforms.Compose(\n",
    "    [transforms.Resize((352, 352)), transforms.ToTensor()]\n",
    ")\n",
    "preprocess_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "dataset_path = \"biomed-clip-share/example_data/biomed_image_classification_example_data\"\n",
    "template = \"this is a photo of \"\n",
    "labels = [\n",
    "    \"adenocarcinoma histopathology\",\n",
    "    \"brain MRI\",\n",
    "    \"covid line chart\",\n",
    "    \"squamous cell carcinoma histopathology\",\n",
    "    \"immunohistochemistry histopathology\",\n",
    "    \"bone X-ray\",\n",
    "    \"chest X-ray\",\n",
    "    \"pie chart\",\n",
    "    \"hematoxylin and eosin histopathology\",\n",
    "]\n",
    "import glob\n",
    "\n",
    "test_imgs = glob.glob(dataset_path + \"/*\")\n",
    "\n",
    "context_length = 256\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "images = torch.stack(\n",
    "    [preprocess_val(Image.open(img).convert(\"RGB\")) for img in test_imgs]\n",
    ").to(device)\n",
    "texts = tokenizer([template + l for l in labels], context_length=context_length).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "normalize = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    features_pyramid = []\n",
    "    # x = model.visual.trunk(images)\n",
    "    x, vit_hidden_states = forward_vit(\n",
    "        model.visual.trunk, images, output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    image_features = model.visual.head(x)\n",
    "    # print(len(hidden_activations))\n",
    "    # print(hidden_activations[-1].shape)\n",
    "    # print(x == _x)\n",
    "\n",
    "    # # x = trunk.forward_features(images)\n",
    "    # # x = trunk(images)\n",
    "    # # print(x[:,1,:], trunk(images))\n",
    "\n",
    "    # x = image_represent(images)\n",
    "    # for block in model.visual.trunk.blocks:\n",
    "    #     x = block(x)\n",
    "    #     features_pyramid.append(x)\n",
    "    # x = model.visual.trunk.norm(x)\n",
    "    # x = model.visual.trunk.fc_norm(x)\n",
    "    # x = model.visual.trunk.head(x)\n",
    "\n",
    "    # x = x[:,-1,:]\n",
    "    # x = model.visual.head(x)\n",
    "    # print(model.visual.trunk.forward_features(images))\n",
    "    # print(x == model.visual.trunk.forward_features(images)[:,1:,:])\n",
    "    # image_features, text_features, logit_scale = model(images, texts)\n",
    "    # image_features = x\n",
    "\n",
    "    text_features, bert_hidden_states = forward_bert(model.text, texts)\n",
    "    if normalize:\n",
    "        image_features, text_features = F.normalize(image_features), F.normalize(\n",
    "            text_features\n",
    "        )\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "\n",
    "    logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n",
    "    sorted_indices = torch.argsort(logits, dim=-1, descending=True)\n",
    "\n",
    "    logits = logits.cpu().numpy()\n",
    "    sorted_indices = sorted_indices.cpu().numpy()\n",
    "\n",
    "top_k = -1\n",
    "\n",
    "for i, img in enumerate(test_imgs):\n",
    "    pred = labels[sorted_indices[i][0]]\n",
    "\n",
    "    top_k = len(labels) if top_k == -1 else top_k\n",
    "    print(img.split(\"/\")[-1] + \":\")\n",
    "    for j in range(top_k):\n",
    "        jth_index = sorted_indices[i][j]\n",
    "        print(f\"{labels[jth_index]}: {logits[i][jth_index]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "\n",
    "class BiomedCLIPSeg(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        extract_layers: List[int] = [3, 6, 9, 11],\n",
    "        reduce_dim=64,\n",
    "        n_heads=4,\n",
    "        extra_blocks=0,\n",
    "        image_size=224,\n",
    "        mask_size=224,\n",
    "    ) -> None:\n",
    "        super(BiomedCLIPSeg, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.mask_size = mask_size\n",
    "\n",
    "        # Timm contiual pretraining\n",
    "        biomedclip_model_name = \"PubMedBERT_256-timm-vit_base_patch16_224\"\n",
    "        checkpoint = \"biomed-clip-share/models/2022_11_08-07_39_28-model_timm-vit_base_patch16_224-lr_0.0005-b_1024-j_8-p_amp/checkpoints/epoch_32.pt\"\n",
    "\n",
    "        (\n",
    "            self.biomedclip_model,\n",
    "            self.transform_train,\n",
    "            self.transform_val,\n",
    "        ) = open_clip.create_model_and_transforms(biomedclip_model_name)\n",
    "        checkpoint = torch.load(checkpoint, map_location=\"cpu\")\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(biomedclip_model_name)\n",
    "        self.context_length = 256\n",
    "\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in checkpoint[\"state_dict\"].items():\n",
    "            new_k = k.replace(\"module.\", \"\")\n",
    "            new_state_dict[new_k] = v\n",
    "        self.biomedclip_model.load_state_dict(\n",
    "            new_state_dict, strict=False\n",
    "        )  # can set this to be true except for timm models\n",
    "\n",
    "        for p in self.biomedclip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Projections to aggregate the text embeddings\n",
    "        self.film_mul = nn.Linear(512, reduce_dim)\n",
    "        self.film_add = nn.Linear(512, reduce_dim)\n",
    "\n",
    "        # Decoder parts\n",
    "        self.extract_layers = extract_layers + []\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.reducers = nn.ModuleList(\n",
    "            [nn.Linear(768, reduce_dim) for _ in extract_layers]\n",
    "        )\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=reduce_dim, nhead=n_heads, batch_first=True\n",
    "                )\n",
    "                for _ in extract_layers\n",
    "            ]\n",
    "        )\n",
    "        self.extra_blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=reduce_dim, nhead=n_heads, batch_first=True\n",
    "                )\n",
    "                for _ in range(extra_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Projection to generate the binary mask\n",
    "        self.conv_trans = nn.ConvTranspose2d(reduce_dim, 1, 16, 16)\n",
    "\n",
    "    def _forward_vit(self, x, hidden_activations: bool = True):\n",
    "        ViT = self.biomedclip_model.visual.trunk\n",
    "        x = ViT.patch_embed(x)\n",
    "        x = ViT._pos_embed(x)\n",
    "        x = ViT.norm_pre(x)\n",
    "\n",
    "        hidden_activations = []\n",
    "\n",
    "        for i, block in enumerate(ViT.blocks):\n",
    "            x = block(x)\n",
    "\n",
    "            if i in self.extract_layers:\n",
    "                hidden_activations.append(x)\n",
    "\n",
    "        x = ViT.norm(x)\n",
    "\n",
    "        if ViT.global_pool:\n",
    "            x = (\n",
    "                x[:, ViT.num_prefix_tokens :].mean(dim=1)\n",
    "                if ViT.global_pool == \"avg\"\n",
    "                else x[:, 0]\n",
    "            )\n",
    "        x = ViT.fc_norm(x)\n",
    "        x = ViT.head(x)\n",
    "\n",
    "        # Linear Projection: 768 -> 512\n",
    "        x = self.biomedclip_model.visual.head(x)\n",
    "\n",
    "        if hidden_activations:\n",
    "            return x, hidden_activations\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        images_embeds, _activations = self._forward_vit(images)\n",
    "        texts_embeds = self.biomedclip_model.text(texts)\n",
    "\n",
    "        _activations = _activations[::-1]\n",
    "        a = None\n",
    "        for i, (activation, block, reducer) in enumerate(\n",
    "            zip(_activations[1:], self.blocks, self.reducers)\n",
    "        ):\n",
    "            if a is not None:\n",
    "                a = reducer(activation) + a\n",
    "            else:\n",
    "                a = reducer(activation)\n",
    "\n",
    "            if i == 0:\n",
    "                a = self.film_mul(texts_embeds)[:, None].repeat(\n",
    "                    1, 197, 1\n",
    "                ) * a + self.film_add(texts_embeds)[:, None].repeat(1, 197, 1)\n",
    "\n",
    "            a = block(a)\n",
    "\n",
    "        for block in self.extra_blocks:\n",
    "            a = a + block(a)\n",
    "\n",
    "        # Discard the CLS token and (*, Token, Feature) -> (*, Feature, Token)\n",
    "        a = a[:, 1:].permute(0, 2, 1)\n",
    "\n",
    "        size = int(math.sqrt(a.shape[2]))\n",
    "\n",
    "        a = a.view(-1, a.shape[1], size, size)\n",
    "\n",
    "        a = self.conv_trans(a)\n",
    "\n",
    "        # Return mask logits\n",
    "        return a\n",
    "\n",
    "\n",
    "dataset_path = \"biomed-clip-share/example_data/biomed_image_classification_example_data\"\n",
    "template = \"this is a photo of \"\n",
    "labels = [\n",
    "    \"adenocarcinoma histopathology\",\n",
    "    \"brain MRI\",\n",
    "    \"covid line chart\",\n",
    "    \"squamous cell carcinoma histopathology\",\n",
    "    \"immunohistochemistry histopathology\",\n",
    "    \"bone X-ray\",\n",
    "    \"chest X-ray\",\n",
    "    \"pie chart\",\n",
    "    \"hematoxylin and eosin histopathology\",\n",
    "]\n",
    "import glob\n",
    "\n",
    "test_imgs = glob.glob(dataset_path + \"/*\")\n",
    "\n",
    "model = BiomedCLIPSeg()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "images = torch.stack([transform_val(Image.open(img)) for img in test_imgs]).to(device)\n",
    "texts = tokenizer([template + l for l in labels], context_length=context_length).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "model(images, texts)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n",
    "import open_clip\n",
    "\n",
    "prompts = [\"medium polyp\", \"image of colon\"]\n",
    "\n",
    "biomedclip_model = open_clip.create_model(\n",
    "    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    ")\n",
    "biomedclip_tokenizer = open_clip.get_tokenizer(\n",
    "    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    ")\n",
    "\n",
    "biomedclip_tokens = biomedclip_tokenizer(prompts, context_length=256)\n",
    "\n",
    "processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "url = (\n",
    "    \"/home/manish/sedoc/biomedclip/lightning-hydra-starter/data/images/bkai-polyp/1.jpg\"\n",
    ")\n",
    "image = Image.open(url)\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    text=prompts,\n",
    "    images=[image] * len(prompts),\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = processor(\n",
    "    text=prompts[0], images=image, padding=\"max_length\", return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs.pixel_values.shape)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "preds = outputs.logits\n",
    "preds = preds.sigmoid().round()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(preds.shape)\n",
    "plt.imshow(np.array(preds))\n",
    "# processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPSegForImageSegmentation, CLIPSegConfig\n",
    "from torch import nn\n",
    "import json\n",
    "\n",
    "decoder = CLIPSegForImageSegmentation(\n",
    "    CLIPSegConfig.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    ").decoder\n",
    "for p in decoder.parameters():\n",
    "    print(p)\n",
    "    break\n",
    "\n",
    "decoder = CLIPSegForImageSegmentation.from_pretrained(\n",
    "    \"CIDAS/clipseg-rd64-refined\"\n",
    ").decoder\n",
    "for p in decoder.parameters():\n",
    "    print(p)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPSegConfig.from_pretrained(\"CIDAS/clipseg-rd64-refined\").extract_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPSegForImageSegmentation, CLIPSegConfig\n",
    "from transformers.models.clipseg.modeling_clipseg import (\n",
    "    CLIPSegOutput,\n",
    "    CLIPSegImageSegmentationOutput,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "import torch\n",
    "from torch import nn\n",
    "import open_clip\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "\n",
    "class BiomedCLIPSeg(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        biomedclip_hf_api: str,\n",
    "        clip_seg_hf_api: str,\n",
    "        freeze_encoder=True,\n",
    "        freeze_decoder=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.clip_seg = CLIPSegForImageSegmentation.from_pretrained(clip_seg_hf_api)\n",
    "        self.biomedclip = open_clip.create_model(biomedclip_hf_api)\n",
    "        self.clip = self.clip_seg.clip\n",
    "        self.decoder = self.clip_seg.decoder\n",
    "        self.get_conditional_embeddings = self.clip_seg.get_conditional_embeddings\n",
    "        self.config = self.clip_seg.config\n",
    "        self.extract_layers = self.clip_seg.extract_layers\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for p in self.biomedclip.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        if freeze_decoder:\n",
    "            for p in self.clip_seg.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def _forward_vit(self, x, output_hidden_states: bool = True):\n",
    "        ViT = self.biomedclip.visual.trunk\n",
    "        x = ViT.patch_embed(x)\n",
    "        x = ViT._pos_embed(x)\n",
    "        x = ViT.norm_pre(x)\n",
    "\n",
    "        # if 0 in self.extract_layers:\n",
    "        #     hidden_states = [x]\n",
    "        # else:\n",
    "        #     hidden_states = []\n",
    "\n",
    "        hidden_states = []\n",
    "\n",
    "        for i, block in enumerate(ViT.blocks):\n",
    "            x = block(x)\n",
    "\n",
    "            # if i+1 in self.extract_layers:\n",
    "            #     hidden_states.append(x)\n",
    "\n",
    "            hidden_states.append(x)\n",
    "\n",
    "        x = ViT.norm(x)\n",
    "\n",
    "        if ViT.global_pool:\n",
    "            x = (\n",
    "                x[:, ViT.num_prefix_tokens :].mean(dim=1)\n",
    "                if ViT.global_pool == \"avg\"\n",
    "                else x[:, 0]\n",
    "            )\n",
    "        x = ViT.fc_norm(x)\n",
    "        x = ViT.head(x)\n",
    "\n",
    "        # Linear Projection: 768 -> 512\n",
    "        x = self.biomedclip.visual.head(x)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            return x, hidden_states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _forward_bert(self, x, output_hidden_states: bool = True):\n",
    "        bert = self.biomedclip.text\n",
    "        attn_mask = (x != bert.config.pad_token_id).long()\n",
    "        out = bert.transformer(\n",
    "            input_ids=x,\n",
    "            attention_mask=attn_mask,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        pooled_out = bert.pooler(out, attn_mask)\n",
    "        projected = bert.proj(pooled_out)\n",
    "\n",
    "        hidden_states = [out.hidden_states[i] for i in self.cond_layers]\n",
    "\n",
    "        seq_len = out.last_hidden_state.shape[1]\n",
    "        tokens = (\n",
    "            out.last_hidden_state[\n",
    "                :, torch.arange(seq_len) != bert.pooler.cls_token_position, :\n",
    "            ]\n",
    "            if type(bert.pooler) == ClsPooler\n",
    "            else out.last_hidden_state\n",
    "        )\n",
    "\n",
    "        if bert.output_tokens:\n",
    "            return projected, tokens\n",
    "\n",
    "        if output_hidden_states:\n",
    "            return projected, hidden_states\n",
    "        else:\n",
    "            return projected\n",
    "\n",
    "    def get_conditional_embeddings(\n",
    "        self,\n",
    "        batch_size: int = None,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        conditional_pixel_values: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            # compute conditional embeddings from texts\n",
    "            if len(input_ids) != batch_size:\n",
    "                raise ValueError(\n",
    "                    \"Make sure to pass as many prompt texts as there are query images\"\n",
    "                )\n",
    "            with torch.no_grad():\n",
    "                conditional_embeddings = self._forward_bert(\n",
    "                    input_ids, output_hidden_states=False\n",
    "                )\n",
    "        elif conditional_pixel_values is not None:\n",
    "            # compute conditional embeddings from images\n",
    "            if len(conditional_pixel_values) != batch_size:\n",
    "                raise ValueError(\n",
    "                    \"Make sure to pass as many prompt images as there are query images\"\n",
    "                )\n",
    "            with torch.no_grad():\n",
    "                conditional_embeddings = self._forward_vit(\n",
    "                    conditional_pixel_values, output_hidden_states=False\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values`\"\n",
    "            )\n",
    "\n",
    "        return conditional_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.FloatTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        conditional_pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        conditional_embeddings: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CLIPSegOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoProcessor, CLIPSegForImageSegmentation\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "        >>> model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "        >>> texts = [\"a cat\", \"a remote\", \"a blanket\"]\n",
    "        >>> inputs = processor(text=texts, images=[image] * len(texts), padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "\n",
    "        >>> logits = outputs.logits\n",
    "        >>> print(logits.shape)\n",
    "        torch.Size([3, 352, 352])\n",
    "        ```\"\"\"\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # step 1: forward the query images through the frozen CLIP vision encoder\n",
    "        with torch.no_grad():\n",
    "            # vision_outputs = self.clip.vision_model(\n",
    "            #     pixel_values=pixel_values,\n",
    "            #     output_attentions=output_attentions,\n",
    "            #     output_hidden_states=True,  # we need the intermediate hidden states\n",
    "            #     return_dict=return_dict,\n",
    "            # )\n",
    "\n",
    "            # pooled_output = self.clip.visual_projection(vision_outputs[1])\n",
    "            # print(vision_outputs.last_hidden_state == vision_outputs.hidden_states[-1])\n",
    "\n",
    "            # hidden_states = vision_outputs.hidden_states if return_dict else vision_outputs[2]\n",
    "            # print(len(hidden_states))\n",
    "\n",
    "            pooled_output, hidden_states = self._forward_vit(\n",
    "                pixel_values, output_hidden_states=True\n",
    "            )\n",
    "            # we add +1 here as the hidden states also include the initial embeddings\n",
    "            activations = [hidden_states[i + 1] for i in self.extract_layers]\n",
    "\n",
    "            # update vision_outputs\n",
    "            # if return_dict:\n",
    "            #     vision_outputs = BaseModelOutputWithPooling(\n",
    "            #         last_hidden_state=vision_outputs.last_hidden_state,\n",
    "            #         pooler_output=vision_outputs.pooler_output,\n",
    "            #         hidden_states=vision_outputs.hidden_states if output_hidden_states else None,\n",
    "            #         attentions=vision_outputs.attentions,\n",
    "            #     )\n",
    "            # else:\n",
    "            #     vision_outputs = (\n",
    "            #         vision_outputs[:2] + vision_outputs[3:] if not output_hidden_states else vision_outputs\n",
    "            #     )\n",
    "\n",
    "        # step 2: compute conditional embeddings, either from text, images or an own provided embedding\n",
    "        if conditional_embeddings is None:\n",
    "            conditional_embeddings = self.get_conditional_embeddings(\n",
    "                batch_size=pixel_values.shape[0],\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                conditional_pixel_values=conditional_pixel_values,\n",
    "            )\n",
    "        else:\n",
    "            if conditional_embeddings.shape[0] != pixel_values.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Make sure to pass as many conditional embeddings as there are query images in the batch\"\n",
    "                )\n",
    "            if conditional_embeddings.shape[1] != self.config.projection_dim:\n",
    "                raise ValueError(\n",
    "                    \"Make sure that the feature dimension of the conditional embeddings matches\"\n",
    "                    \" `config.projection_dim`.\"\n",
    "                )\n",
    "\n",
    "        # step 3: forward both the pooled output and the activations through the lightweight decoder to predict masks\n",
    "        decoder_outputs = self.decoder(\n",
    "            activations,\n",
    "            conditional_embeddings,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        logits = decoder_outputs.logits if return_dict else decoder_outputs[0]\n",
    "\n",
    "        return logits[:, None]\n",
    "        # loss = None\n",
    "        # if labels is not None:\n",
    "        #     # move labels to the correct device to enable PP\n",
    "        #     labels = labels.to(logits.device)\n",
    "        #     loss_fn = nn.BCEWithLogitsLoss()\n",
    "        #     loss = loss_fn(logits, labels)\n",
    "\n",
    "        # if not return_dict:\n",
    "        #     output = (logits, conditional_embeddings, pooled_output, vision_outputs, decoder_outputs)\n",
    "        #     return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        # return CLIPSegImageSegmentationOutput(\n",
    "        #     loss=loss,\n",
    "        #     logits=logits,\n",
    "        #     conditional_embeddings=conditional_embeddings,\n",
    "        #     pooled_output=pooled_output,\n",
    "        #     vision_model_output=vision_outputs,\n",
    "        #     decoder_output=decoder_outputs,\n",
    "        # )\n",
    "\n",
    "\n",
    "pixel_values = torch.rand((32, 3, 224, 224))\n",
    "input_ids = torch.randint(0, 200, (32, 77))\n",
    "\n",
    "biomedclip_seg = BiomedCLIPSeg(\n",
    "    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n",
    "    \"CIDAS/clipseg-rd64-refined\",\n",
    ")\n",
    "biomedclip_seg(input_ids, pixel_values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPSegProcessor\n",
    "import open_clip\n",
    "\n",
    "prompts = [\"one small sized polyps\"]\n",
    "\n",
    "biomedclip_model = open_clip.create_model(\n",
    "    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    ")\n",
    "biomedclip_tokenizer = open_clip.get_tokenizer(\n",
    "    \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    ")\n",
    "\n",
    "biomedclip_tokens = biomedclip_tokenizer(prompts, context_length=256)\n",
    "\n",
    "processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "model = BiomedCLIPSeg.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "url = (\n",
    "    \"/home/manish/sedoc/biomedclip/lightning-hydra-starter/data/images/bkai-polyp/0.jpg\"\n",
    ")\n",
    "image = Image.open(url)\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    text=prompts,\n",
    "    images=[image] * len(prompts),\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "preds = outputs.logits\n",
    "preds = preds.sigmoid().round()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(preds.shape)\n",
    "plt.imshow(np.array(preds))\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPSegForImageSegmentation\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class CLIPSeg(nn.Module):\n",
    "    def __init__(self, hf_api: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_seg = CLIPSegForImageSegmentation.from_pretrained(hf_api)\n",
    "\n",
    "    def forward(self, input_ids, pixel_values):\n",
    "        outputs = self.clip_seg(input_ids, pixel_values)\n",
    "        return outputs.logits\n",
    "\n",
    "\n",
    "clip_seg = CLIPSeg(\"CIDAS/clipseg-rd64-refined\")\n",
    "\n",
    "torch.save(\n",
    "    clip_seg.state_dict(),\n",
    "    \"/home/manish/sedoc/biomedclip/lightning-hydra-starter/logs/pretrained/CLIPSeg.ckpt\",\n",
    ")\n",
    "# clip_seg.load_state_dict(dict(clip_seg=torch.load(\"/home/manish/.cache/huggingface/hub/models--CIDAS--clipseg-rd64-refined/snapshots/583b388deb98a04feb3e1f816dcdb8f3062ee205/pytorch_model.bin\")))\n",
    "# torch.load(\"/home/manish/.cache/huggingface/hub/models--CIDAS--clipseg-rd64-refined/snapshots/583b388deb98a04feb3e1f816dcdb8f3062ee205/pytorch_model.bin\")\n",
    "# torch.load(\"/home/manish/sedoc/biomedclip/lightning-hydra-starter/logs/pretrained/CLIPSeg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n",
    "\n",
    "processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://unsplash.com/photos/8Nc_oQsc2qQ/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjcxMjAwNzI0&force=true&w=640\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "from albumentations import Normalize, Resize, Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "prompts = [\"cutlery\", \"pancakes\", \"blueberries\", \"orange juice\"]\n",
    "images = [\n",
    "    Compose([Resize(352, 352), Normalize(), ToTensorV2()])(image=np.array(image))[\n",
    "        \"image\"\n",
    "    ]\n",
    "] * len(prompts)\n",
    "images = torch.stack(images)\n",
    "\n",
    "inputs = processor(\n",
    "    text=prompts,\n",
    "    images=[image] * len(prompts),\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        pixel_values=images,\n",
    "    )\n",
    "preds = outputs.logits.unsqueeze(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots(1, len(prompts) + 1, figsize=(3 * (len(prompts) + 1), 4))\n",
    "[a.axis(\"off\") for a in ax.flatten()]\n",
    "ax[0].imshow(image)\n",
    "[ax[i + 1].imshow(torch.sigmoid(preds[i][0])) for i in range(len(prompts))]\n",
    "[ax[i + 1].text(0, -15, prompt) for i, prompt in enumerate(prompts)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import Normalize, RandomResizedCrop\n",
    "import numpy as np\n",
    "\n",
    "out1 = Normalize()(image=np.array(image))\n",
    "out1[\"image\"].max()\n",
    "out2 = Normalize()(image=out1[\"image\"])\n",
    "out1[\"image\"].max(), out2[\"image\"].max()\n",
    "RandomResizedCrop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "tokenizer.encode(\n",
    "    \"i am good\",\n",
    "    max_length=12,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the datasets used for VLSMs pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/manish/sedoc/biomedclip/lightning-hydra-starter/phrasecut/refer_train.json\", \"r\") as fp:\n",
    "    json_data = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocab from text\n",
    "from collections import Counter\n",
    "all_text = \"\"\n",
    "for c in json_data:\n",
    "    all_text += c['phrase'] + \" \"\n",
    "counter = Counter(all_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('white', 22696),\n",
       " ('on', 16536),\n",
       " ('black', 13408),\n",
       " ('blue', 10752),\n",
       " ('green', 10170),\n",
       " ('man', 9726),\n",
       " ('sky', 8677),\n",
       " ('brown', 8291),\n",
       " ('in', 7817),\n",
       " ('wall', 7641),\n",
       " ('of', 7279),\n",
       " ('shirt', 7005),\n",
       " ('red', 6872),\n",
       " ('building', 6159),\n",
       " ('tree', 6062),\n",
       " ('grass', 5132),\n",
       " ('woman', 4874),\n",
       " ('ground', 4837),\n",
       " ('table', 4526),\n",
       " ('water', 4491),\n",
       " ('person', 4134),\n",
       " ('has', 4073),\n",
       " ('yellow', 4013),\n",
       " ('trees', 3775),\n",
       " ('large', 3731),\n",
       " ('window', 3678),\n",
       " ('sign', 3605),\n",
       " ('wearing', 3179),\n",
       " ('gray', 3090),\n",
       " ('head', 3059),\n",
       " ('wooden', 3005),\n",
       " ('pole', 2985),\n",
       " ('street', 2983),\n",
       " ('fence', 2942),\n",
       " ('train', 2942),\n",
       " ('road', 2875),\n",
       " ('grey', 2853),\n",
       " ('hair', 2814),\n",
       " ('a', 2809),\n",
       " ('floor', 2753),\n",
       " ('light', 2728),\n",
       " ('pants', 2678),\n",
       " ('plate', 2609),\n",
       " ('metal', 2454),\n",
       " ('people', 2444),\n",
       " ('door', 2342),\n",
       " ('small', 2267),\n",
       " ('shadow', 2240),\n",
       " ('dark', 2212),\n",
       " ('clouds', 2161)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phrase_structure': {'attributes': ['wounded up'], 'type': 'name', 'name': 'hose', 'relation_descriptions': [], 'relation_ids': []}, 'task_id': '2393830__469500', 'instance_boxes': [[363.3333333333333, 280.8333333333333, 102.66666666666669, 50.166666666666686]], 'image_id': 2393830, 'ann_ids': [469500], 'phrase': 'wounded up hose', 'Polygons': [[[[425.8333333333333, 281.6666666666667], [400.0, 280.8333333333333], [375.83333333333337, 285.8333333333333], [363.3333333333333, 298.3333333333333], [372.5, 319.16666666666663], [391.6666666666667, 330.0], [432.5, 329.1666666666667], [445.0, 329.1666666666667], [456.6666666666667, 322.5], [447.5, 309.16666666666663], [465.0, 304.16666666666663], [455.8333333333333, 291.6666666666667], [445.0, 285.8333333333333]]]]}\n",
      "{'phrase_structure': {'attributes': ['wound'], 'type': 'name', 'name': 'hose', 'relation_descriptions': [], 'relation_ids': []}, 'task_id': '2405970__327536', 'instance_boxes': [[389.1666666666667, 3.3333333333333335, 110.16666666666669, 146.83333333333331]], 'image_id': 2405970, 'ann_ids': [327536], 'phrase': 'wound hose', 'Polygons': [[[[422.5, 10.833333333333334], [461.6666666666667, 3.3333333333333335], [496.66666666666663, 6.666666666666667], [498.33333333333337, 130.0], [496.66666666666663, 141.66666666666666], [486.6666666666667, 149.16666666666666], [469.1666666666667, 149.16666666666666], [451.6666666666667, 125.83333333333333], [427.5, 117.5], [394.1666666666667, 95.83333333333334], [389.1666666666667, 69.16666666666667], [394.1666666666667, 50.0], [401.6666666666667, 30.833333333333336]]]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_phrases = []\n",
    "for c in json_data:\n",
    "    if \"wound\" in c[\"phrase\"].lower():\n",
    "        print(c)\n",
    "        relevant_phrases.append(c[\"phrase\"])\n",
    "len(relevant_phrases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageMask Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=\".\",\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules.datasets import ImageMaskDataset\n",
    "ds = ImageMaskDataset(dataset_dir=\"/home/manish/sedoc/biomedclip/lightning-hydra-starter/data/combined_datasets/\", dataset_name=\"chexlocalize\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fee7d1b59f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mask = ds[0][\"mask\"] \n",
    "print(mask.shape)\n",
    "plt.imshow(mask[15])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.metrics.meandice import compute_dice\n",
    "import torch\n",
    "input = torch.rand((1, 1, 352, 352))\n",
    "target = torch.zeros_like(target)\n",
    "compute_dice( input, target, ignore_empty=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9677, 0.7976, 0.2418,  ..., 0.4350, 0.1871, 0.4091],\n",
       "          [0.0438, 0.7347, 0.1341,  ..., 0.4527, 0.5314, 0.1878],\n",
       "          [0.1945, 0.0616, 0.6207,  ..., 0.7440, 0.2759, 0.3921],\n",
       "          ...,\n",
       "          [0.4614, 0.7087, 0.0161,  ..., 0.3705, 0.2344, 0.2941],\n",
       "          [0.1402, 0.3929, 0.5823,  ..., 0.0399, 0.2415, 0.7379],\n",
       "          [0.0531, 0.6212, 0.2361,  ..., 0.5032, 0.0740, 0.1791]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manish/sedoc/medvlsm/.venv/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/manish/sedoc/medvlsm/.venv/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manish/sedoc/medvlsm/.venv/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"/home/manish/sedoc/medvlsm/data/isic/images/0.jpg\").convert(\"RGB\")\n",
    "mask = Image.open(\"/home/manish/sedoc/medvlsm/data/isic/masks/0.png\")\n",
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(size=(500, 500)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = transforms(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8276,  0.8276,  0.8276,  ..., -0.0629, -0.0458, -0.0458],\n",
       "         [ 0.8276,  0.8276,  0.8276,  ..., -0.0801, -0.0629, -0.0801],\n",
       "         [ 0.8276,  0.8276,  0.8276,  ..., -0.0116, -0.0287, -0.0458],\n",
       "         ...,\n",
       "         [ 0.7077,  0.6734,  0.6734,  ..., -0.4739, -0.4568, -0.4226],\n",
       "         [ 0.6906,  0.6906,  0.6906,  ..., -0.4397, -0.3883, -0.3369],\n",
       "         [ 0.7248,  0.7077,  0.6906,  ..., -0.4054, -0.3027, -0.1828]],\n",
       "\n",
       "        [[ 1.6408,  1.6408,  1.6408,  ..., -0.1800, -0.1625, -0.1625],\n",
       "         [ 1.6408,  1.6408,  1.6408,  ..., -0.1800, -0.1800, -0.1800],\n",
       "         [ 1.6408,  1.6408,  1.6408,  ..., -0.1099, -0.1275, -0.1450],\n",
       "         ...,\n",
       "         [ 1.3081,  1.3256,  1.3256,  ..., -0.5651, -0.5476, -0.5126],\n",
       "         [ 1.3431,  1.3431,  1.3431,  ..., -0.5301, -0.4776, -0.4251],\n",
       "         [ 1.3782,  1.3782,  1.3606,  ..., -0.4951, -0.3901, -0.2675]],\n",
       "\n",
       "        [[ 2.5006,  2.5006,  2.5006,  ..., -0.0092,  0.0431,  0.0082],\n",
       "         [ 2.5006,  2.5006,  2.5006,  ..., -0.0615, -0.0092, -0.0615],\n",
       "         [ 2.5006,  2.5006,  2.5006,  ...,  0.0082, -0.0092, -0.0267],\n",
       "         ...,\n",
       "         [ 2.1346,  2.1346,  2.1346,  ..., -0.3404, -0.3230, -0.2881],\n",
       "         [ 2.1520,  2.1520,  2.1520,  ..., -0.3055, -0.2532, -0.2010],\n",
       "         [ 2.1868,  2.2217,  2.2043,  ..., -0.2707, -0.1661, -0.0441]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 500])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79c236746b1b2eb4ad392af6d23a10828cb422ab548ef5be6252fda8c002b87e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
